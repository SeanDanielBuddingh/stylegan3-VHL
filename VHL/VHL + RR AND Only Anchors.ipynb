{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 917,
      "metadata": {
        "id": "hEjVqEOAbBqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 918,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOYC9dkzbMau",
        "outputId": "41c6e0f8-0eb3-47f0-ffd3-7319e89d1bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "raw_train = torchvision.datasets.CIFAR10('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "raw_test = torchvision.datasets.CIFAR10('./', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(raw_test, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 919,
      "metadata": {
        "id": "KNboi_FZbOzo"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "n_samples = 6000 #number of fake images in the virutal dataset per class (same as CIFAR10)\n",
        "\n",
        "random_state = np.random.default_rng(seed=42)  # for reproducibility\n",
        "indices2targets = [(i, target) for i, (_, target) in enumerate(raw_train)]\n",
        "non_iid_alpha = 0.1\n",
        "num_classes = 10  # CIFAR10 has 10 classes\n",
        "num_indices = len(raw_train)\n",
        "n_workers = 10\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 920,
      "metadata": {
        "id": "vsleDJ41bTY3"
      },
      "outputs": [],
      "source": [
        "#dirichlet sampling function from VHL codebase\n",
        "def build_non_iid_by_dirichlet(\n",
        "    random_state, indices2targets, non_iid_alpha, num_classes, num_indices, n_workers\n",
        "):\n",
        "    n_auxi_workers = 10\n",
        "    assert n_auxi_workers <= n_workers\n",
        "\n",
        "    # random shuffle targets indices.\n",
        "    random_state.shuffle(indices2targets)\n",
        "\n",
        "    # partition indices.\n",
        "    from_index = 0\n",
        "    splitted_targets = []\n",
        "    num_splits = math.ceil(n_workers / n_auxi_workers)\n",
        "    split_n_workers = [\n",
        "        n_auxi_workers\n",
        "        if idx < num_splits - 1\n",
        "        else n_workers - n_auxi_workers * (num_splits - 1)\n",
        "        for idx in range(num_splits)\n",
        "    ]\n",
        "    split_ratios = [_n_workers / n_workers for _n_workers in split_n_workers]\n",
        "    for idx, ratio in enumerate(split_ratios):\n",
        "        to_index = from_index + int(n_auxi_workers / n_workers * num_indices)\n",
        "        splitted_targets.append(\n",
        "            indices2targets[\n",
        "                from_index : (num_indices if idx == num_splits - 1 else to_index)\n",
        "            ]\n",
        "        )\n",
        "        from_index = to_index\n",
        "\n",
        "    #\n",
        "    idx_batch = []\n",
        "    for _targets in splitted_targets:\n",
        "        # rebuild _targets.\n",
        "        _targets = np.array(_targets)\n",
        "        _targets_size = len(_targets)\n",
        "\n",
        "        # use auxi_workers for this subset targets.\n",
        "        _n_workers = min(n_auxi_workers, n_workers)\n",
        "        n_workers = n_workers - n_auxi_workers\n",
        "\n",
        "        # get the corresponding idx_batch.\n",
        "        min_size = 0\n",
        "        while min_size < int(0.50 * _targets_size / _n_workers):\n",
        "            _idx_batch = [[] for _ in range(_n_workers)]\n",
        "            for _class in range(num_classes):\n",
        "                # get the corresponding indices in the original 'targets' list.\n",
        "                idx_class = np.where(_targets[:, 1] == _class)[0]\n",
        "                idx_class = _targets[idx_class, 0]\n",
        "\n",
        "                # sampling.\n",
        "                try:\n",
        "                    proportions = random_state.dirichlet(\n",
        "                        np.repeat(non_iid_alpha, _n_workers)\n",
        "                    )\n",
        "                    # balance\n",
        "                    proportions = np.array(\n",
        "                        [\n",
        "                            p * (len(idx_j) < _targets_size / _n_workers)\n",
        "                            for p, idx_j in zip(proportions, _idx_batch)\n",
        "                        ]\n",
        "                    )\n",
        "                    proportions = proportions / proportions.sum()\n",
        "                    proportions = (np.cumsum(proportions) * len(idx_class)).astype(int)[\n",
        "                        :-1\n",
        "                    ]\n",
        "                    _idx_batch = [\n",
        "                        idx_j + idx.tolist()\n",
        "                        for idx_j, idx in zip(\n",
        "                            _idx_batch, np.split(idx_class, proportions)\n",
        "                        )\n",
        "                    ]\n",
        "                    sizes = [len(idx_j) for idx_j in _idx_batch]\n",
        "                    min_size = min([_size for _size in sizes])\n",
        "                except ZeroDivisionError:\n",
        "                    pass\n",
        "        idx_batch += _idx_batch\n",
        "    return idx_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 921,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rFDSEviCZ8T",
        "outputId": "3724343c-f00e-40a6-9d82-02a24f0baef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3854\n",
            "5081\n",
            "5357\n",
            "3645\n",
            "5361\n",
            "5378\n",
            "3247\n",
            "5167\n",
            "5095\n",
            "7815\n"
          ]
        }
      ],
      "source": [
        "idx_batch = build_non_iid_by_dirichlet(random_state, indices2targets, non_iid_alpha, num_classes, num_indices, n_workers)\n",
        "for i, idx in enumerate(idx_batch):\n",
        "  print(len(idx))\n",
        "client_datasets = [torch.utils.data.DataLoader(raw_train, batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(idx)) for idx in idx_batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 922,
      "metadata": {
        "id": "p9g16xwPS5j0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, args=None, image_size=32, model_input_channels=3, device=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.args = args\n",
        "        self.image_size = image_size\n",
        "        self.device = device\n",
        "\n",
        "        self.conv1 = nn.Conv2d(model_input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.layers_name_map = {\n",
        "            \"classifier\": \"linear\"\n",
        "        }\n",
        "\n",
        "        inplanes = [64, 64, 128, 256, 512]\n",
        "        inplanes = [ inplane * block.expansion for inplane in inplanes]\n",
        "        #logging.info(inplanes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    #forward pass copied from resnet in VHL codebase, if statements removed. instead of selecting a layer for feature output, i return features for all layers\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        feat1 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer2(out)\n",
        "        feat2 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer3(out)\n",
        "        feat3 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer4(out)\n",
        "        feat4 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.avgpool(out)\n",
        "        feat_final = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.linear(out.view(out.size(0), -1))\n",
        "\n",
        "        return out, feat_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 923,
      "metadata": {
        "id": "PzBj3-2V1Nea"
      },
      "outputs": [],
      "source": [
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, latent_dim, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=in_channels, out_channels=hidden_channels * 2, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.LayerNorm([hidden_channels * 2, 32, 32]),  # Layer normalization instead of batch normalization\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Conv2d(in_channels=hidden_channels * 2, out_channels=hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.LayerNorm([hidden_channels, 16, 16]),  # Layer normalization instead of batch normalization\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(hidden_channels * (8 * 8), latent_dim)\n",
        "        )\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim, latent_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(latent_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      feat = self.encoder(x)\n",
        "      out = self.decoder(feat)\n",
        "\n",
        "      return out, feat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 924,
      "metadata": {
        "id": "c-uDxLA3LlUE"
      },
      "outputs": [],
      "source": [
        "class SupConLoss(nn.Module):\n",
        "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
        "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
        "    def __init__(self, contrast_mode='all',\n",
        "                base_temperature=0.07, device=None):\n",
        "        super(SupConLoss, self).__init__()\n",
        "\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, features, labels=None, temperature=0.07, mask=None):\n",
        "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
        "        it degenerates to SimCLR unsupervised loss:\n",
        "        https://arxiv.org/pdf/2002.05709.pdf\n",
        "        Args:\n",
        "            features: hidden vector of shape [bsz, n_views, ...].\n",
        "            labels: ground truth of shape [bsz].\n",
        "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                has the same class as sample i. Can be asymmetric.\n",
        "        Returns:\n",
        "            A loss scalar.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(self.device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float().to(self.device)\n",
        "        else:\n",
        "            mask = mask.float().to(self.device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T), temperature)\n",
        "\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(self.device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        exp_logits = ( torch.exp(logits) * logits_mask ) + 0.0001\n",
        "\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        mask_sum = mask.sum(1)\n",
        "        mask_sum[mask_sum == 0] += 1\n",
        "\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_sum\n",
        "\n",
        "        loss = - (temperature / self.base_temperature) * mean_log_prob_pos\n",
        "\n",
        "        if torch.any(torch.isnan(loss)):\n",
        "            raise RuntimeError\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 926,
      "metadata": {
        "id": "QmiPjfzIppQn"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "    def __init__(self, client_id, datasets, device):\n",
        "        self.client_id = client_id\n",
        "        self.train_dataset, self.test_dataset = datasets\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def id(self):\n",
        "        return self.client_id\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def local_update(self, E, LR, MOMENTUM, BETA, LOG_INTERVAL, virtual_dataset, anchors_tensor, etf):\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "        self.model.to(self.device)\n",
        "        supcon = SupConLoss(device=self.device)\n",
        "        anchor_1_channels = []\n",
        "        anchor_2_channels = []\n",
        "        anchor_3_channels = []\n",
        "\n",
        "        # Split each anchor into its individual channels\n",
        "        for anchor in anchors_tensor:\n",
        "            anchor_1, anchor_2, anchor_3 = torch.split(anchor, 1, dim=0)\n",
        "\n",
        "            anchor_1_channels.append(anchor_1)\n",
        "            anchor_2_channels.append(anchor_2)\n",
        "            anchor_3_channels.append(anchor_3)\n",
        "\n",
        "        anchor_1_channels = torch.stack(anchor_1_channels)\n",
        "        anchor_2_channels = torch.stack(anchor_2_channels)\n",
        "        anchor_3_channels = torch.stack(anchor_3_channels)\n",
        "\n",
        "        i=0\n",
        "\n",
        "        #there is now just one batch for the virtual dataset of size 10\n",
        "        for virtual_inputs, virtual_targets in virtual_dataset:\n",
        "            continue\n",
        "        \n",
        "        for epoch in range(1, E + 1):\n",
        "            clf_losses, info_losses, accs = [], [], []\n",
        "            for inputs, targets in self.train_dataset:\n",
        "                i+=1\n",
        "\n",
        "                sim_list = []\n",
        "                for (input, target) in zip(inputs, targets):\n",
        "                    channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                    sim_1 = torch.nn.functional.cosine_similarity(channel_1.to(self.device), anchor_1_channels[target].to(self.device), dim=0)\n",
        "                    sim_2 = torch.nn.functional.cosine_similarity(channel_2.to(self.device), anchor_2_channels[target].to(self.device), dim=0)\n",
        "                    sim_3 = torch.nn.functional.cosine_similarity(channel_3.to(self.device), anchor_3_channels[target].to(self.device), dim=0)\n",
        "\n",
        "                    sim_stack = torch.stack((sim_1.squeeze(0), sim_2.squeeze(0), sim_3.squeeze(0)))\n",
        "                    sim_list.append(sim_stack)\n",
        "\n",
        "                inputs = torch.stack(sim_list)\n",
        "\n",
        "                sim_list = []\n",
        "                for (input, target) in zip(virtual_inputs, virtual_targets):\n",
        "                    channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                    sim_1 = torch.nn.functional.cosine_similarity(channel_1.to(self.device), anchor_1_channels[target].to(self.device), dim=0)\n",
        "                    sim_2 = torch.nn.functional.cosine_similarity(channel_2.to(self.device), anchor_2_channels[target].to(self.device), dim=0)\n",
        "                    sim_3 = torch.nn.functional.cosine_similarity(channel_3.to(self.device), anchor_3_channels[target].to(self.device), dim=0)\n",
        "\n",
        "                    sim_stack = torch.stack((sim_1.squeeze(0), sim_2.squeeze(0), sim_3.squeeze(0)))\n",
        "                    sim_list.append(sim_stack)\n",
        "\n",
        "                virtual_inputs = torch.stack(sim_list)\n",
        "\n",
        "                inputs, targets = inputs.to(self.device), targets.long().to(self.device)\n",
        "                virtual_inputs, virtual_targets = virtual_inputs.to(self.device), virtual_targets.long().to(self.device)\n",
        "\n",
        "                outputs, feat = self.model(inputs)\n",
        "                virtual_outputs, vfeat = self.model(virtual_inputs)\n",
        "\n",
        "                natural_loss = F.cross_entropy(outputs, targets)\n",
        "                virtual_loss = F.cross_entropy(virtual_outputs, virtual_targets)\n",
        "\n",
        "                stacked_features = torch.cat((feat, vfeat.detach()))\n",
        "                stacked_targets = torch.cat((targets, virtual_targets))\n",
        "                if torch.any(torch.isnan(stacked_features)):\n",
        "                  print('features: ', torch.any(torch.isnan(stacked_features)))\n",
        "                discrepancy = supcon(features=stacked_features.unsqueeze(1), labels=stacked_targets)\n",
        "\n",
        "                loss = natural_loss + virtual_loss + (BETA * discrepancy) \n",
        "\n",
        "                if torch.isnan(natural_loss).any() or torch.isinf(natural_loss).any():\n",
        "                    print('cause is natural ',loss)\n",
        "                if torch.isnan(virtual_loss).any() or torch.isinf(virtual_loss).any():\n",
        "                    print('cause is virtual ',loss)\n",
        "                if torch.isnan(discrepancy).any() or torch.isinf(discrepancy).any():\n",
        "                    print('cause is discrepancy. nans or infs perpetuated through the supcon function. ',loss)\n",
        "                acc = torch.eq(outputs.max(1)[1], targets).float().mean()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                clf_losses.append(natural_loss.item())\n",
        "\n",
        "                accs.append(acc.item())\n",
        "\n",
        "            avg_clf_loss = torch.tensor(clf_losses).mean().item()\n",
        "            avg_acc = torch.tensor(accs).mean().item()\n",
        "            if epoch % LOG_INTERVAL == 0:\n",
        "              print(f\"(CLIENT {self.client_id}) [UPDATE] [EPOCH {epoch}] Classification Loss: {avg_clf_loss:.4f} | Acc: {avg_acc:.4f}\")\n",
        "        self.model.to('cpu')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def local_evaluate(self, anchors_tensor):\n",
        "        self.model.to(self.device)\n",
        "        supcon = SupConLoss()\n",
        "\n",
        "        anchor_1_channels = []\n",
        "        anchor_2_channels = []\n",
        "        anchor_3_channels = []\n",
        "\n",
        "        # Split each anchor into its individual channels\n",
        "        for anchor in anchors_tensor:\n",
        "            anchor_1, anchor_2, anchor_3 = torch.split(anchor, 1, dim=0)\n",
        "\n",
        "            anchor_1_channels.append(anchor_1)\n",
        "            anchor_2_channels.append(anchor_2)\n",
        "            anchor_3_channels.append(anchor_3)\n",
        "\n",
        "        anchor_1_channels = torch.stack(anchor_1_channels)\n",
        "        anchor_2_channels = torch.stack(anchor_2_channels)\n",
        "        anchor_3_channels = torch.stack(anchor_3_channels)\n",
        "\n",
        "        clf_losses, info_losses, accs = [], [], []\n",
        "        for inputs, targets in self.test_dataset:\n",
        "            \n",
        "            sim_list = []\n",
        "            for (input, target) in zip(inputs, targets):\n",
        "\n",
        "                channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                sim_1 = torch.nn.functional.cosine_similarity(channel_1, anchor_1_channels, dim=0)\n",
        "                sim_2 = torch.nn.functional.cosine_similarity(channel_2, anchor_2_channels, dim=0)\n",
        "                sim_3 = torch.nn.functional.cosine_similarity(channel_3, anchor_3_channels, dim=0)\n",
        "\n",
        "                sim_stack = torch.stack((sim_1.squeeze(0), sim_2.squeeze(0), sim_3.squeeze(0)))\n",
        "\n",
        "                sim_list.append(sim_stack)\n",
        "            \n",
        "            inputs = torch.stack(sim_list)\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            outputs, feat = self.model(inputs)\n",
        "\n",
        "            class_loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            loss = class_loss\n",
        "            acc = torch.eq(outputs.max(1)[1], targets).float().mean()\n",
        "\n",
        "            clf_losses.append(class_loss.item())\n",
        "\n",
        "            accs.append(acc.item())\n",
        "\n",
        "        avg_clf_loss = torch.tensor(clf_losses).mean().item()\n",
        "\n",
        "        avg_acc = torch.tensor(accs).mean().item()\n",
        "        print(f\"(CLIENT {self.client_id}) [EVALUATE] Classification Loss: {avg_clf_loss:.4f} | Acc: {avg_acc:.4f}\\n\")\n",
        "        self.model.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 927,
      "metadata": {
        "id": "xtg7drqObSVV"
      },
      "outputs": [],
      "source": [
        "import copy \n",
        "\n",
        "NUM_CLASSES = 10\n",
        "ANCHORS_PER_CLASS = 20\n",
        "IN_CHANNELS = 3\n",
        "HIDDEN_CHANNELS = 16\n",
        "LATENT_DIM = 200\n",
        "# ResNet Latent Dim\n",
        "#LATENT_DIM = 512\n",
        "\n",
        "placeholder = ''\n",
        "server = Client('SERVER_MODEL', (placeholder, test_loader), DEVICE)\n",
        "#server.model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "server.model = ConvNet(IN_CHANNELS, HIDDEN_CHANNELS, LATENT_DIM, NUM_CLASSES)\n",
        "\n",
        "clients = {}\n",
        "for i in range(10):\n",
        "    clients[i] = Client(i, (client_datasets[i], test_loader), DEVICE)\n",
        "    #clients[i].model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "    clients[i].model = copy.deepcopy(server.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 928,
      "metadata": {
        "id": "5_HPzK_6DBvG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "\n",
        "class NoiseDataset(Dataset):\n",
        "    def __init__(self, output_dir, num_samples=6000, num_classes=10, shape=(3, 32, 32)):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_classes = num_classes\n",
        "        self.shape = shape\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        for i in range(num_classes):\n",
        "            c = (np.random.randn(num_samples // 10, *shape).astype(np.float32) * 0.02) + (i * 0.1)\n",
        "            self.data.append(c)\n",
        "            self.targets.extend([i] * (num_samples // 10))\n",
        "\n",
        "            # Save the generated images\n",
        "            for j in range(num_samples // 10):\n",
        "                image = c[j]\n",
        "                image_path = os.path.join(self.output_dir, f\"class_{i}\", f\"image_{j}.png\")\n",
        "                os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
        "                torchvision.utils.save_image(torch.tensor(image), image_path)\n",
        "\n",
        "        self.data = np.concatenate(self.data)\n",
        "        self.targets = np.array(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_samples // 10) * self.num_classes\n",
        "\n",
        "# Example usage:\n",
        "#output_dir = \"virtual_dataset_images\"\n",
        "#virtual_dataset = NoiseDataset(output_dir=output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 929,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilizes StyleGAN3 to generate images for each class, based on gaussion noise inputs\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "sys.path.append('../')\n",
        "\n",
        "from training.networks_stylegan3 import Generator\n",
        "\n",
        "def Generate_StyleGAN_Images():\n",
        "    # Define the parameters\n",
        "    z_dim = 512\n",
        "    c_dim = 0\n",
        "    w_dim = 512\n",
        "    img_resolution = 32\n",
        "    img_channels = 3\n",
        "\n",
        "    # Instantiate the Generator\n",
        "    generator = Generator(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels)\n",
        "\n",
        "    # Create folder\n",
        "    output_dir = \"StyleGAN_images\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate and save images for each class\n",
        "    class_samples = 1  # Number of images to generate for each distribution\n",
        "    num_distributions = 10  # Number of Gaussian distributions\n",
        "    batch_size = 1\n",
        "\n",
        "    # Generate Gaussian noise\n",
        "    means = torch.arange(0, num_distributions) \n",
        "    print(means)\n",
        "    noise = torch.stack([torch.randn(z_dim) for _ in range(10)])\n",
        "\n",
        "    for i in range(num_distributions):\n",
        "        x = 0\n",
        "        class_input = noise[i]\n",
        "\n",
        "        class_vectors = []\n",
        "        for _ in range(class_samples):\n",
        "            change = torch.randn(1, z_dim) * 0.1\n",
        "            new_input = class_input + change\n",
        "            class_vectors.append(new_input)\n",
        "\n",
        "        class_vectors_tensor = torch.stack(class_vectors).squeeze(1)\n",
        "\n",
        "        # Batch the vectors for generation\n",
        "        batched_class_vectors = torch.split(class_vectors_tensor, batch_size)\n",
        "\n",
        "        for vectors in batched_class_vectors:\n",
        "            # Generate images\n",
        "            with torch.no_grad():\n",
        "                generated_images = generator(vectors, None)\n",
        "        \n",
        "            # Save images in subfolders for different classes\n",
        "            class_output_dir = os.path.join(output_dir, f\"class_{i}\")\n",
        "            os.makedirs(class_output_dir, exist_ok=True)\n",
        "        \n",
        "            for j in range(batch_size):\n",
        "\n",
        "                image_path = os.path.join(class_output_dir, f\"image_{j+x}.png\")\n",
        "                generated_image_np = (generated_images[j].detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "                generated_image_np = generated_image_np.transpose(1, 2, 0)\n",
        "                \n",
        "                image = Image.fromarray(generated_image_np)\n",
        "                image.save(image_path)\n",
        "\n",
        "            x+=10\n",
        "\n",
        "    print(\"Images saved successfully.\")\n",
        "    return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 930,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class GeneratedImageDataset(Dataset):\n",
        "    def __init__(self, folder_path, num_classes):\n",
        "        self.folder_path = folder_path\n",
        "        self.num_classes = num_classes\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_idx in range(num_classes):\n",
        "            class_folder = os.path.join(self.folder_path, f\"class_{class_idx}\")\n",
        "            image_files = os.listdir(class_folder)\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(class_folder, image_file)\n",
        "                self.image_paths.append(image_path)\n",
        "                self.labels.append(class_idx)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "        transform = transforms.ToTensor()\n",
        "        image = transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 931,
      "metadata": {
        "id": "qw6423zpP4hg"
      },
      "outputs": [],
      "source": [
        "#virtual_dataset = NoiseDataset()\n",
        "#client_virtual = torch.utils.data.DataLoader(virtual_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#make_imgs = Generate_StyleGAN_Images()\n",
        "generated_dataset = GeneratedImageDataset(folder_path='StyleGAN_images', num_classes=10)\n",
        "client_virtual = torch.utils.data.DataLoader(generated_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 932,
      "metadata": {
        "id": "7Tf2XuytBaZY"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_latent(model, anchors, X, y):\n",
        "    x_tensor = torch.stack(X)\n",
        "    y_tensor = torch.Tensor(y)\n",
        "\n",
        "    anchor_1_channels = []\n",
        "    anchor_2_channels = []\n",
        "    anchor_3_channels = []\n",
        "\n",
        "    # Split each anchor into its individual channels\n",
        "    for anchor in anchors:\n",
        "        anchor_1, anchor_2, anchor_3 = torch.split(anchor, 1, dim=0)\n",
        "\n",
        "        anchor_1_channels.append(anchor_1)\n",
        "        anchor_2_channels.append(anchor_2)\n",
        "        anchor_3_channels.append(anchor_3)\n",
        "\n",
        "    anchor_1_channels = torch.stack(anchor_1_channels)\n",
        "    anchor_2_channels = torch.stack(anchor_2_channels)\n",
        "    anchor_3_channels = torch.stack(anchor_3_channels)\n",
        "\n",
        "    sim_list = []\n",
        "    for inputs, target in zip(x_tensor, y_tensor):\n",
        "        channel_1, channel_2, channel_3 = torch.split(inputs, 1, dim=0)\n",
        "        target = int(target)\n",
        "        sim_1 = torch.nn.functional.cosine_similarity(channel_1, anchor_1_channels, dim=0)\n",
        "        sim_2 = torch.nn.functional.cosine_similarity(channel_2, anchor_2_channels, dim=0)\n",
        "        sim_3 = torch.nn.functional.cosine_similarity(channel_3, anchor_3_channels, dim=0)\n",
        "\n",
        "        sim_stack = torch.stack((sim_1.squeeze(0), sim_2.squeeze(0), sim_3.squeeze(0)))\n",
        "        sim_list.append(sim_stack)\n",
        "\n",
        "    inputs = torch.stack(sim_list)\n",
        "        \n",
        "    Z = model(inputs)[1].numpy()\n",
        "    #Z = model(x_tensor)[1].numpy()\n",
        "    Z = TSNE().fit_transform(X=Z, y=y_tensor)\n",
        "    plt.scatter(Z[:, 0], Z[:, 1], c=y, cmap='tab10')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 933,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_etf(LATENT_DIM, NUM_CLASSES):\n",
        "    A = torch.randn(LATENT_DIM, NUM_CLASSES)\n",
        "    P, _ = torch.linalg.qr(A)\n",
        "    I = torch.eye(NUM_CLASSES)\n",
        "    O = torch.ones(NUM_CLASSES, NUM_CLASSES)\n",
        "    M = torch.matmul(P, I.sub(O.mul(1 / NUM_CLASSES))).mul((NUM_CLASSES / (NUM_CLASSES - 1))**0.5)\n",
        "    return M.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 934,
      "metadata": {},
      "outputs": [],
      "source": [
        "etf = generate_etf(LATENT_DIM, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 935,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4vOvzJAWBc4",
        "outputId": "85b4ed9d-9933-4bcf-cffa-e95887fea862"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import random\n",
        "from itertools import zip_longest\n",
        "\n",
        "E = 50\n",
        "LOG_INTERVAL = 1\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BETA = 1e-3\n",
        "total_samples = 60010\n",
        "communication_rounds = 200\n",
        "num_clients = 10\n",
        "\n",
        "X = [raw_test[i][0] for i in range(len(raw_test))]\n",
        "Y = [raw_test[i][1] for i in range(len(raw_test))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 936,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Server defines an anchor\n",
        "# Utilizes samples generated from the StyleGAN for anchors\n",
        "anchors = []\n",
        "\n",
        "for class_label in range(num_clients):\n",
        "    sample_index = next(index for index, (data, label) in enumerate(generated_dataset) if label == class_label)\n",
        "    image_tensor = generated_dataset[sample_index][0]\n",
        "    image_np = image_tensor.numpy()\n",
        "    image_float32 = image_np.astype(np.float32)\n",
        "    # Add a small amount of noise to the image to create anchor\n",
        "    anchor = torch.Tensor(image_float32 + np.random.randn(*(3, 32, 32)).astype(np.float32) * 0.01)\n",
        "    anchors.append(anchor)\n",
        "\n",
        "anchors_tensor = torch.stack(anchors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 937,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data_weights = [(len(clients[i].train_dataset.dataset) + 6000) for i in clients]\n",
        "total_weight = sum(data_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 938,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "(CLIENT 0) [UPDATE] [EPOCH 1] Classification Loss: 1.0287 | Acc: 0.4761\n",
            "(CLIENT 0) [UPDATE] [EPOCH 2] Classification Loss: 0.9024 | Acc: 0.5158\n",
            "(CLIENT 0) [UPDATE] [EPOCH 3] Classification Loss: 0.8764 | Acc: 0.5420\n",
            "(CLIENT 0) [UPDATE] [EPOCH 4] Classification Loss: 0.8516 | Acc: 0.5569\n",
            "(CLIENT 0) [UPDATE] [EPOCH 5] Classification Loss: 0.8512 | Acc: 0.5500\n",
            "(CLIENT 0) [UPDATE] [EPOCH 6] Classification Loss: 0.8358 | Acc: 0.5725\n",
            "(CLIENT 0) [UPDATE] [EPOCH 7] Classification Loss: 0.8245 | Acc: 0.5785\n",
            "(CLIENT 0) [UPDATE] [EPOCH 8] Classification Loss: 0.8060 | Acc: 0.5903\n",
            "(CLIENT 0) [UPDATE] [EPOCH 9] Classification Loss: 0.7839 | Acc: 0.5980\n",
            "(CLIENT 0) [UPDATE] [EPOCH 10] Classification Loss: 0.7858 | Acc: 0.6030\n",
            "(CLIENT 0) [UPDATE] [EPOCH 11] Classification Loss: 0.7918 | Acc: 0.5928\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for rounds in range(communication_rounds):\n",
        "    print(rounds)\n",
        "    for i in range(num_clients):\n",
        "        clients[i].wasserstein_sums = []\n",
        "        clients[i].local_update(E, LR, MOMENTUM, BETA, LOG_INTERVAL, client_virtual, anchors_tensor, etf)\n",
        "        clients[i].local_evaluate(anchors_tensor)\n",
        "        clients[i].data_weight = data_weights[i] / total_weight\n",
        "\n",
        "    # FedAVG implementation\n",
        "    averaged_weights = OrderedDict()\n",
        "    client_keys = list(clients.keys())  \n",
        "    selected_keys = random.sample(client_keys, 10) \n",
        "\n",
        "    for key in server.model.state_dict().keys():\n",
        "        averaged_weights[key] = sum(clients[client_key].model.state_dict()[key] * clients[client_key].data_weight for client_key in selected_keys)\n",
        "\n",
        "    server.model.load_state_dict(averaged_weights)\n",
        "    plot_latent(server.model, anchors, X, Y)\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        clients[i].model.load_state_dict(server.model.state_dict())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
