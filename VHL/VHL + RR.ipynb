{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hEjVqEOAbBqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOYC9dkzbMau",
        "outputId": "41c6e0f8-0eb3-47f0-ffd3-7319e89d1bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "raw_train = torchvision.datasets.CIFAR10('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "raw_test = torchvision.datasets.CIFAR10('./', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(raw_test, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KNboi_FZbOzo"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "n_samples = 6000 #number of fake images in the virutal dataset per class (same as CIFAR10)\n",
        "\n",
        "random_state = np.random.default_rng(seed=42)  # for reproducibility\n",
        "indices2targets = [(i, target) for i, (_, target) in enumerate(raw_train)]\n",
        "non_iid_alpha = 0.1\n",
        "num_classes = 10  # CIFAR10 has 10 classes\n",
        "num_indices = len(raw_train)\n",
        "n_workers = 10\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vsleDJ41bTY3"
      },
      "outputs": [],
      "source": [
        "#dirichlet sampling function from VHL codebase\n",
        "def build_non_iid_by_dirichlet(\n",
        "    random_state, indices2targets, non_iid_alpha, num_classes, num_indices, n_workers\n",
        "):\n",
        "    n_auxi_workers = 10\n",
        "    assert n_auxi_workers <= n_workers\n",
        "\n",
        "    # random shuffle targets indices.\n",
        "    random_state.shuffle(indices2targets)\n",
        "\n",
        "    # partition indices.\n",
        "    from_index = 0\n",
        "    splitted_targets = []\n",
        "    num_splits = math.ceil(n_workers / n_auxi_workers)\n",
        "    split_n_workers = [\n",
        "        n_auxi_workers\n",
        "        if idx < num_splits - 1\n",
        "        else n_workers - n_auxi_workers * (num_splits - 1)\n",
        "        for idx in range(num_splits)\n",
        "    ]\n",
        "    split_ratios = [_n_workers / n_workers for _n_workers in split_n_workers]\n",
        "    for idx, ratio in enumerate(split_ratios):\n",
        "        to_index = from_index + int(n_auxi_workers / n_workers * num_indices)\n",
        "        splitted_targets.append(\n",
        "            indices2targets[\n",
        "                from_index : (num_indices if idx == num_splits - 1 else to_index)\n",
        "            ]\n",
        "        )\n",
        "        from_index = to_index\n",
        "\n",
        "    #\n",
        "    idx_batch = []\n",
        "    for _targets in splitted_targets:\n",
        "        # rebuild _targets.\n",
        "        _targets = np.array(_targets)\n",
        "        _targets_size = len(_targets)\n",
        "\n",
        "        # use auxi_workers for this subset targets.\n",
        "        _n_workers = min(n_auxi_workers, n_workers)\n",
        "        n_workers = n_workers - n_auxi_workers\n",
        "\n",
        "        # get the corresponding idx_batch.\n",
        "        min_size = 0\n",
        "        while min_size < int(0.50 * _targets_size / _n_workers):\n",
        "            _idx_batch = [[] for _ in range(_n_workers)]\n",
        "            for _class in range(num_classes):\n",
        "                # get the corresponding indices in the original 'targets' list.\n",
        "                idx_class = np.where(_targets[:, 1] == _class)[0]\n",
        "                idx_class = _targets[idx_class, 0]\n",
        "\n",
        "                # sampling.\n",
        "                try:\n",
        "                    proportions = random_state.dirichlet(\n",
        "                        np.repeat(non_iid_alpha, _n_workers)\n",
        "                    )\n",
        "                    # balance\n",
        "                    proportions = np.array(\n",
        "                        [\n",
        "                            p * (len(idx_j) < _targets_size / _n_workers)\n",
        "                            for p, idx_j in zip(proportions, _idx_batch)\n",
        "                        ]\n",
        "                    )\n",
        "                    proportions = proportions / proportions.sum()\n",
        "                    proportions = (np.cumsum(proportions) * len(idx_class)).astype(int)[\n",
        "                        :-1\n",
        "                    ]\n",
        "                    _idx_batch = [\n",
        "                        idx_j + idx.tolist()\n",
        "                        for idx_j, idx in zip(\n",
        "                            _idx_batch, np.split(idx_class, proportions)\n",
        "                        )\n",
        "                    ]\n",
        "                    sizes = [len(idx_j) for idx_j in _idx_batch]\n",
        "                    min_size = min([_size for _size in sizes])\n",
        "                except ZeroDivisionError:\n",
        "                    pass\n",
        "        idx_batch += _idx_batch\n",
        "    return idx_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rFDSEviCZ8T",
        "outputId": "3724343c-f00e-40a6-9d82-02a24f0baef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3854\n",
            "5081\n",
            "5357\n",
            "3645\n",
            "5361\n",
            "5378\n",
            "3247\n",
            "5167\n",
            "5095\n",
            "7815\n"
          ]
        }
      ],
      "source": [
        "idx_batch = build_non_iid_by_dirichlet(random_state, indices2targets, non_iid_alpha, num_classes, num_indices, n_workers)\n",
        "for i, idx in enumerate(idx_batch):\n",
        "  print(len(idx))\n",
        "client_datasets = [torch.utils.data.DataLoader(raw_train, batch_size=32, sampler=torch.utils.data.SubsetRandomSampler(idx)) for idx in idx_batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p9g16xwPS5j0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, args=None, image_size=32, model_input_channels=3, device=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.args = args\n",
        "        self.image_size = image_size\n",
        "        self.device = device\n",
        "\n",
        "        self.conv1 = nn.Conv2d(model_input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.layers_name_map = {\n",
        "            \"classifier\": \"linear\"\n",
        "        }\n",
        "\n",
        "        inplanes = [64, 64, 128, 256, 512]\n",
        "        inplanes = [ inplane * block.expansion for inplane in inplanes]\n",
        "        #logging.info(inplanes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    #forward pass copied from resnet in VHL codebase, if statements removed. instead of selecting a layer for feature output, i return features for all layers\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        feat1 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer2(out)\n",
        "        feat2 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer3(out)\n",
        "        feat3 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.layer4(out)\n",
        "        feat4 = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.avgpool(out)\n",
        "        feat_final = out.view(out.size(0), -1) * 1.0\n",
        "        out = self.linear(out.view(out.size(0), -1))\n",
        "\n",
        "        return out, feat_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PzBj3-2V1Nea"
      },
      "outputs": [],
      "source": [
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, latent_dim, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=in_channels, out_channels=hidden_channels * 2, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.LayerNorm([hidden_channels * 2, 32, 32]),  # Layer normalization instead of batch normalization\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Conv2d(in_channels=hidden_channels * 2, out_channels=hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.LayerNorm([hidden_channels, 16, 16]),  # Layer normalization instead of batch normalization\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(hidden_channels * (8 * 8), latent_dim)\n",
        "        )\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim, latent_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(latent_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      feat = self.encoder(x)\n",
        "      out = self.decoder(feat)\n",
        "\n",
        "      return out, feat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c-uDxLA3LlUE"
      },
      "outputs": [],
      "source": [
        "class SupConLoss(nn.Module):\n",
        "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
        "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
        "    def __init__(self, contrast_mode='all',\n",
        "                base_temperature=0.07, device=None):\n",
        "        super(SupConLoss, self).__init__()\n",
        "\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, features, labels=None, temperature=0.07, mask=None):\n",
        "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
        "        it degenerates to SimCLR unsupervised loss:\n",
        "        https://arxiv.org/pdf/2002.05709.pdf\n",
        "        Args:\n",
        "            features: hidden vector of shape [bsz, n_views, ...].\n",
        "            labels: ground truth of shape [bsz].\n",
        "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                has the same class as sample i. Can be asymmetric.\n",
        "        Returns:\n",
        "            A loss scalar.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32).to(self.device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float().to(self.device)\n",
        "        else:\n",
        "            mask = mask.float().to(self.device)\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T), temperature)\n",
        "\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1).to(self.device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        exp_logits = ( torch.exp(logits) * logits_mask ) + 0.0001\n",
        "\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        mask_sum = mask.sum(1)\n",
        "        mask_sum[mask_sum == 0] += 1\n",
        "\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_sum\n",
        "\n",
        "        loss = - (temperature / self.base_temperature) * mean_log_prob_pos\n",
        "\n",
        "        if torch.any(torch.isnan(loss)):\n",
        "            raise RuntimeError\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QmiPjfzIppQn"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "    def __init__(self, client_id, datasets, device):\n",
        "        self.client_id = client_id\n",
        "        self.train_dataset, self.test_dataset = datasets\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def id(self):\n",
        "        return self.client_id\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def local_update(self, E, LR, MOMENTUM, BETA, LOG_INTERVAL, virtual_dataset, anchors_tensor):\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "        self.model.to(self.device)\n",
        "        supcon = SupConLoss()\n",
        "        anchor_1_channels = []\n",
        "        anchor_2_channels = []\n",
        "        anchor_3_channels = []\n",
        "\n",
        "        # Split each anchor into its individual channels\n",
        "        for anchor in anchors_tensor:\n",
        "            anchor_1, anchor_2, anchor_3 = torch.split(anchor, 1, dim=0)\n",
        "\n",
        "            anchor_1_channels.append(anchor_1)\n",
        "            anchor_2_channels.append(anchor_2)\n",
        "            anchor_3_channels.append(anchor_3)\n",
        "\n",
        "        i=0\n",
        "        for epoch in range(1, E + 1):\n",
        "            clf_losses, info_losses, accs = [], [], []\n",
        "            for (inputs, targets), (virtual_inputs, virtual_targets) in zip(self.train_dataset, virtual_dataset):\n",
        "                i+=1\n",
        "\n",
        "                sim_list = []\n",
        "                for (input, target) in zip(inputs, targets):\n",
        "                    channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                    sim_1 = torch.nn.functional.cosine_similarity(channel_1, anchor_1_channels[target], dim=0)\n",
        "                    sim_2 = torch.nn.functional.cosine_similarity(channel_2, anchor_2_channels[target], dim=0)\n",
        "                    sim_3 = torch.nn.functional.cosine_similarity(channel_3, anchor_3_channels[target], dim=0)\n",
        "\n",
        "                    sim_stack = torch.stack((sim_1, sim_2, sim_3))\n",
        "                    sim_list.append(sim_stack)\n",
        "\n",
        "                inputs = torch.stack(sim_list)\n",
        "\n",
        "                sim_list = []\n",
        "                for (input, target) in zip(virtual_inputs, virtual_targets):\n",
        "                    channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                    sim_1 = torch.nn.functional.cosine_similarity(channel_1, anchor_1_channels[target], dim=0)\n",
        "                    sim_2 = torch.nn.functional.cosine_similarity(channel_2, anchor_2_channels[target], dim=0)\n",
        "                    sim_3 = torch.nn.functional.cosine_similarity(channel_3, anchor_3_channels[target], dim=0)\n",
        "\n",
        "                    sim_stack = torch.stack((sim_1, sim_2, sim_3))\n",
        "                    sim_list.append(sim_stack)\n",
        "\n",
        "                virtual_inputs = torch.stack(sim_list)\n",
        "\n",
        "                inputs, targets = inputs.to(self.device), targets.long().to(self.device)\n",
        "                virtual_inputs, virtual_targets = virtual_inputs.to(self.device), virtual_targets.long().to(self.device)\n",
        "\n",
        "                outputs, feat = self.model(inputs)\n",
        "                virtual_outputs, vfeat = self.model(virtual_inputs)\n",
        "\n",
        "                natural_loss = F.cross_entropy(outputs, targets)\n",
        "                virtual_loss = F.cross_entropy(virtual_outputs, virtual_targets)\n",
        "\n",
        "                stacked_features = torch.cat((feat, vfeat.detach()))\n",
        "                stacked_targets = torch.cat((targets, virtual_targets))\n",
        "                if torch.any(torch.isnan(stacked_features)):\n",
        "                  print('features: ', torch.any(torch.isnan(stacked_features)))\n",
        "                discrepancy = supcon(features=stacked_features.unsqueeze(1), labels=stacked_targets)\n",
        "\n",
        "                loss = natural_loss +  virtual_loss + ( BETA * discrepancy )\n",
        "                if torch.isnan(natural_loss).any() or torch.isinf(natural_loss).any():\n",
        "                    print('cause is natural ',loss)\n",
        "                if torch.isnan(virtual_loss).any() or torch.isinf(virtual_loss).any():\n",
        "                    print('cause is virtual ',loss)\n",
        "                if torch.isnan(discrepancy).any() or torch.isinf(discrepancy).any():\n",
        "                    print('cause is discrepancy. nans or infs perpetuated through the supcon function. ',loss)\n",
        "                acc = torch.eq(outputs.max(1)[1], targets).float().mean()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                clf_losses.append(natural_loss.item())\n",
        "\n",
        "                accs.append(acc.item())\n",
        "\n",
        "            avg_clf_loss = torch.tensor(clf_losses).mean().item()\n",
        "            avg_acc = torch.tensor(accs).mean().item()\n",
        "            if epoch % LOG_INTERVAL == 0:\n",
        "              print(f\"(CLIENT {self.client_id}) [UPDATE] [EPOCH {epoch}] Classification Loss: {avg_clf_loss:.4f} | Acc: {avg_acc:.4f}\")\n",
        "        self.model.to('cpu')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def local_evaluate(self, anchors_tensor):\n",
        "        self.model.to(self.device)\n",
        "        supcon = SupConLoss()\n",
        "\n",
        "        anchor_1_channels = []\n",
        "        anchor_2_channels = []\n",
        "        anchor_3_channels = []\n",
        "\n",
        "        # Split each anchor into its individual channels\n",
        "        for anchor in anchors_tensor:\n",
        "            anchor_1, anchor_2, anchor_3 = torch.split(anchor, 1, dim=0)\n",
        "\n",
        "            anchor_1_channels.append(anchor_1)\n",
        "            anchor_2_channels.append(anchor_2)\n",
        "            anchor_3_channels.append(anchor_3)\n",
        "\n",
        "        clf_losses, info_losses, accs = [], [], []\n",
        "        for inputs, targets in self.test_dataset:\n",
        "            sim_list = []\n",
        "            for (input, target) in zip(inputs, targets):\n",
        "                channel_1, channel_2, channel_3 = torch.split(input, 1, dim=0)\n",
        "                sim_1 = torch.nn.functional.cosine_similarity(channel_1, anchor_1_channels[target], dim=0)\n",
        "                sim_2 = torch.nn.functional.cosine_similarity(channel_2, anchor_2_channels[target], dim=0)\n",
        "                sim_3 = torch.nn.functional.cosine_similarity(channel_3, anchor_3_channels[target], dim=0)\n",
        "\n",
        "                sim_stack = torch.stack((sim_1, sim_2, sim_3))\n",
        "                sim_list.append(sim_stack)\n",
        "\n",
        "            inputs = torch.stack(sim_list)\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            outputs, feat = self.model(inputs)\n",
        "\n",
        "            class_loss = F.cross_entropy(outputs, targets)\n",
        "\n",
        "            loss = class_loss\n",
        "            acc = torch.eq(outputs.max(1)[1], targets).float().mean()\n",
        "\n",
        "            clf_losses.append(class_loss.item())\n",
        "\n",
        "            accs.append(acc.item())\n",
        "\n",
        "        avg_clf_loss = torch.tensor(clf_losses).mean().item()\n",
        "\n",
        "        avg_acc = torch.tensor(accs).mean().item()\n",
        "        print(f\"(CLIENT {self.client_id}) [EVALUATE] Classification Loss: {avg_clf_loss:.4f} | Acc: {avg_acc:.4f}\\n\")\n",
        "        self.model.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xtg7drqObSVV"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "ANCHORS_PER_CLASS = 20\n",
        "IN_CHANNELS = 3\n",
        "HIDDEN_CHANNELS = 16\n",
        "LATENT_DIM = 200\n",
        "\n",
        "clients = {}\n",
        "for i in range(10):\n",
        "    clients[i] = Client(i, (client_datasets[i], test_loader), DEVICE)\n",
        "    #clients[i].model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "    clients[i].model = ConvNet(IN_CHANNELS, HIDDEN_CHANNELS, LATENT_DIM, NUM_CLASSES)\n",
        "\n",
        "placeholder = ''\n",
        "server = Client('SERVER_MODEL', (placeholder, test_loader), DEVICE)\n",
        "#server.model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "server.model = ConvNet(IN_CHANNELS, HIDDEN_CHANNELS, LATENT_DIM, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5_HPzK_6DBvG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NoiseDataset(Dataset):\n",
        "    def __init__(self, num_samples=6000, num_classes=10, shape=(3, 32, 32)):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_classes = num_classes\n",
        "        self.shape = shape\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        for i in range(num_classes):\n",
        "            c = ( np.random.randn(num_samples // 10, *shape).astype(np.float32) * 0.01 ) + ( i * 0.1 )\n",
        "            self.data.append(c)\n",
        "            self.targets.extend([i]*(num_samples // 10))\n",
        "        self.data = np.concatenate(self.data)\n",
        "        self.targets = np.array(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_samples // 10) * self.num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qw6423zpP4hg"
      },
      "outputs": [],
      "source": [
        "virtual_dataset = NoiseDataset()\n",
        "client_virtual = torch.utils.data.DataLoader(virtual_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7Tf2XuytBaZY"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_latent(model, X, y):\n",
        "    x_tensor = torch.stack(X)\n",
        "    y_tensor = torch.Tensor(y)\n",
        "    print(torch.unique(y_tensor))\n",
        "    Z = model.encoder(x_tensor).numpy()\n",
        "    Z = TSNE().fit_transform(X=Z, y=y)\n",
        "    plt.scatter(Z[:, 0], Z[:, 1], c=y, cmap='tab10')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4vOvzJAWBc4",
        "outputId": "85b4ed9d-9933-4bcf-cffa-e95887fea862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 32, 32])\n",
            "0\n",
            "(CLIENT 0) [UPDATE] [EPOCH 1] Classification Loss: 0.5970 | Acc: 0.8723\n",
            "(CLIENT 0) [EVALUATE] Classification Loss: 1.4391 | Acc: 0.4013\n",
            "\n",
            "(CLIENT 1) [UPDATE] [EPOCH 1] Classification Loss: 0.7076 | Acc: 0.8634\n",
            "(CLIENT 1) [EVALUATE] Classification Loss: 1.8882 | Acc: 0.3977\n",
            "\n",
            "(CLIENT 2) [UPDATE] [EPOCH 1] Classification Loss: 1.2420 | Acc: 0.6118\n",
            "(CLIENT 2) [EVALUATE] Classification Loss: 2.5766 | Acc: 0.1010\n",
            "\n",
            "(CLIENT 3) [UPDATE] [EPOCH 1] Classification Loss: 1.7681 | Acc: 0.4071\n",
            "(CLIENT 3) [EVALUATE] Classification Loss: 2.4861 | Acc: 0.1000\n",
            "\n",
            "(CLIENT 4) [UPDATE] [EPOCH 1] Classification Loss: 1.1124 | Acc: 0.7078\n",
            "(CLIENT 4) [EVALUATE] Classification Loss: 1.4119 | Acc: 0.4912\n",
            "\n",
            "(CLIENT 5) [UPDATE] [EPOCH 1] Classification Loss: 0.7632 | Acc: 0.9270\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "import random\n",
        "\n",
        "E = 1\n",
        "LOG_INTERVAL = 1\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BETA = 1e-3\n",
        "total_samples = 66000\n",
        "communication_rounds = 200\n",
        "\n",
        "X = [raw_test[i][0] for i in range(len(raw_test))]\n",
        "Y = [raw_test[i][1] for i in range(len(raw_test))]\n",
        "\n",
        "#Server defines an anchor\n",
        "#Temporarily using CIFAR10 samples per class anchors // this is a problem we will need to solve.\n",
        "# --------------> ETF vectors are made for the latent space dimensions, not input space. ETF vectors may serve as anchors (Theoretical Outputs of a frozen encoder) for Optimal Transfer, but not for Relative Representation.\n",
        "#It seems anchors made of pure noise are useless\n",
        "anchors = []\n",
        "\n",
        "for class_label in range(10):\n",
        "    sample_index = next(index for index, (data, label) in enumerate(raw_train) if label == class_label)\n",
        "    image_tensor = raw_train[sample_index][0]\n",
        "    image_np = image_tensor.numpy()\n",
        "    image_float32 = image_np.astype(np.float32)\n",
        "    # Add a small amount of noise to the image to create anchor\n",
        "    anchor = torch.Tensor(image_float32 + np.random.randn(*(3, 32, 32)).astype(np.float32) * 0.01)\n",
        "    anchors.append(anchor)\n",
        "\n",
        "anchors_tensor = torch.stack(anchors)\n",
        "print(anchors_tensor.shape)\n",
        "data_weights = [(len(clients[i].train_dataset.dataset) + 6000) for i in clients]\n",
        "total_weight = sum(data_weights)\n",
        "\n",
        "for rounds in range(communication_rounds):\n",
        "  print(rounds)\n",
        "  for i in range(10):\n",
        "      clients[i].local_update(E, LR, MOMENTUM, BETA, LOG_INTERVAL, client_virtual, anchors_tensor)\n",
        "      clients[i].local_evaluate(anchors_tensor)\n",
        "      clients[i].data_weight = data_weights[i] / total_weight\n",
        "\n",
        "  averaged_weights = OrderedDict()\n",
        "  client_keys = list(clients.keys())  # Get the list of keys from the clients dictionary\n",
        "  selected_keys = random.sample(client_keys, 10)  # Randomly select X keys\n",
        "\n",
        "  for key in server.model.state_dict().keys():\n",
        "      averaged_weights[key] = sum(clients[client_key].model.state_dict()[key] * clients[client_key].data_weight for client_key in selected_keys)\n",
        "\n",
        "  server.model.load_state_dict(averaged_weights)\n",
        "  plot_latent(server.model, X, Y)\n",
        "\n",
        "  for i in range(10):\n",
        "      clients[i].model.load_state_dict(server.model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nCrmTrGqtGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}